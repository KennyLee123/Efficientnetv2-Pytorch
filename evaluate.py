from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torchvision import datasets, models, transforms
import time
import os

import argparse
from efficientnetv2 import *
# some parameters
use_gpu = torch.cuda.is_available()
print(use_gpu)

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

data_dir = ''
num_epochs = 30
batch_size = 1
input_size = 64
class_num = 1000
weights_loc = "efficientnet-b7 (1).pth"
lr = 0.01
net_name = 'efficientnet-b3'
epoch_to_resume_from = 0
momentum = 0.9


def loaddata(data_dir, batch_size, set_name, shuffle):
    transform_set = [ 
             transforms.GaussianBlur(3),
]
    data_transforms = {
        'train': transforms.Compose([
            transforms.Resize(input_size),
            transforms.CenterCrop(input_size),
            transforms.RandomAffine(degrees=1, translate=(0.05, 0.05)),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(30,  expand=False, center=(55, 5)),
            transforms.RandomApply(transform_set, p=0.5),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
        'test': transforms.Compose([
            transforms.Resize(size=256),
            transforms.CenterCrop(size=224),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ]),
    }

    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in [set_name]}
    # num_workers=0 if CPU else =1
    classs=image_datasets["test"].classes
    dataset_loaders = {x: torch.utils.data.DataLoader(image_datasets[x],
                                                      batch_size=batch_size,
                                                      shuffle=False, num_workers=1) for x in [set_name]}
    data_set_sizes = len(image_datasets[set_name])
    return dataset_loaders, data_set_sizes, classs



def test_model(model, criterion):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    cont = 0
    outPre = []
    outLabel = []
    dset_loaders, dset_sizes,classs= loaddata(data_dir=data_dir, batch_size=batch_size, set_name='test', shuffle=False)
    for data in dset_loaders['test']:
        inputs, labels = data
        labels = torch.squeeze(labels.type(torch.LongTensor))
        labels =Variable(labels.cuda())
        print(labels)
        outputs = model(inputs)
        _, preds = torch.max(outputs.data, 1)
        outPre.append(preds.item())
        #print(outPre)
    return classs,outPre



def exp_lr_scheduler(optimizer, epoch, init_lr=0.01, lr_decay_epoch=10):
    """Decay learning rate by a f#            model_out_path ="./model/W_epoch_{}.pth".format(epoch)
#            torch.save(model_W, model_out_path) actor of 0.1 every lr_decay_epoch epochs."""
    lr = init_lr * (0.8**(epoch // lr_decay_epoch))
    print('LR is set to {}'.format(lr))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr

    return optimizer


def run():
    #build the model
    model = get_efficientnet_v2("efficientnet_v2_m",0)
    #fully connected layer for multi-class classification
    #you can change based on your customized model
    model.head.classifier=nn.Linear(in_features=1280, out_features=1000, bias=True)
    model.classifier = nn.Sequential(
        nn.Linear(in_features=1280,out_features=1300),
        nn.ReLU(),
        nn.Dropout(p=0.8),
        nn.Linear(in_features=1300,out_features=1100),
        nn.ReLU(),
        nn.Linear(in_features=1100,out_features=1000),
    )
    #here we load the model I trained and stopped because of the time restriction on Kaggle
    model.load_state_dict(torch.load("efficientnetv2.pt"))
    import pandas as pd
    criterion = nn.CrossEntropyLoss().cuda()
    answer=[]
    classs,outpre=test_model(model, criterion)
    for i in range(len(classs)):
        classs[i]=classs[i][3:]
    dict={classs[i]:outpre[i] for i in range(len(classs))}
    for i in range(len(classs)):
       answer.append(dict[str(i)])
    map = {'a1_0': 0, 'a1_1': 1, 'a1_10': 2, 'a1_100': 3, 'a1_101': 4, 'a1_102': 5, 'a1_103': 6, 'a1_104': 7, 'a1_105': 8, 'a1_106': 9, 'a1_107': 10, 'a1_108': 11, 'a1_109': 12, 'a1_11': 13, 'a1_110': 14, 'a1_111': 15, 'a1_112': 16, 'a1_113': 17, 'a1_114': 18, 'a1_115': 19, 'a1_116': 20, 'a1_117': 21, 'a1_118': 22, 'a1_119': 23, 'a1_12': 24, 'a1_120': 25, 'a1_121': 26, 'a1_122': 27, 'a1_123': 28, 'a1_124': 29, 'a1_125': 30, 'a1_126': 31, 'a1_127': 32, 'a1_128': 33, 'a1_129': 34, 'a1_13': 35, 'a1_130': 36, 'a1_131': 37, 'a1_132': 38, 'a1_133': 39, 'a1_134': 40, 'a1_135': 41, 'a1_136': 42, 'a1_137': 43, 'a1_138': 44, 'a1_139': 45, 'a1_14': 46, 'a1_140': 47, 'a1_141': 48, 'a1_142': 49, 'a1_143': 50, 'a1_144': 51, 'a1_145': 52, 'a1_146': 53, 'a1_147': 54, 'a1_148': 55, 'a1_149': 56, 'a1_15': 57, 'a1_150': 58, 'a1_151': 59, 'a1_152': 60, 'a1_153': 61, 'a1_154': 62, 'a1_155': 63, 'a1_156': 64, 'a1_157': 65, 'a1_158': 66, 'a1_159': 67, 'a1_16': 68, 'a1_160': 69, 'a1_161': 70, 'a1_162': 71, 'a1_163': 72, 'a1_164': 73, 'a1_165': 74, 'a1_166': 75, 'a1_167': 76, 'a1_168': 77, 'a1_169': 78, 'a1_17': 79, 'a1_170': 80, 'a1_171': 81, 'a1_172': 82, 'a1_173': 83, 'a1_174': 84, 'a1_175': 85, 'a1_176': 86, 'a1_177': 87, 'a1_178': 88, 'a1_179': 89, 'a1_18': 90, 'a1_180': 91, 'a1_181': 92, 'a1_182': 93, 'a1_183': 94, 'a1_184': 95, 'a1_185': 96, 'a1_186': 97, 'a1_187': 98, 'a1_188': 99, 'a1_189': 100, 'a1_19': 101, 'a1_190': 102, 'a1_191': 103, 'a1_192': 104, 'a1_193': 105, 'a1_194': 106, 'a1_195': 107, 'a1_196': 108, 'a1_197': 109, 'a1_198': 110, 'a1_199': 111, 'a1_2': 112, 'a1_20': 113, 'a1_200': 114, 'a1_201': 115, 'a1_202': 116, 'a1_203': 117, 'a1_204': 118, 'a1_205': 119, 'a1_206': 120, 'a1_207': 121, 'a1_208': 122, 'a1_209': 123, 'a1_21': 124, 'a1_210': 125, 'a1_211': 126, 'a1_212': 127, 'a1_213': 128, 'a1_214': 129, 'a1_215': 130, 'a1_216': 131, 'a1_217': 132, 'a1_218': 133, 'a1_219': 134, 'a1_22': 135, 'a1_220': 136, 'a1_221': 137, 'a1_222': 138, 'a1_223': 139, 'a1_224': 140, 'a1_225': 141, 'a1_226': 142, 'a1_227': 143, 'a1_228': 144, 'a1_229': 145, 'a1_23': 146, 'a1_230': 147, 'a1_231': 148, 'a1_232': 149, 'a1_233': 150, 'a1_234': 151, 'a1_235': 152, 'a1_236': 153, 'a1_237': 154, 'a1_238': 155, 'a1_239': 156, 'a1_24': 157, 'a1_240': 158, 'a1_241': 159, 'a1_242': 160, 'a1_243': 161, 'a1_244': 162, 'a1_245': 163, 'a1_246': 164, 'a1_247': 165, 'a1_248': 166, 'a1_249': 167, 'a1_25': 168, 'a1_250': 169, 'a1_251': 170, 'a1_252': 171, 'a1_253': 172, 'a1_254': 173, 'a1_255': 174, 'a1_256': 175, 'a1_257': 176, 'a1_258': 177, 'a1_259': 178, 'a1_26': 179, 'a1_260': 180, 'a1_261': 181, 'a1_262': 182, 'a1_263': 183, 'a1_264': 184, 'a1_265': 185, 'a1_266': 186, 'a1_267': 187, 'a1_268': 188, 'a1_269': 189, 'a1_27': 190, 'a1_270': 191, 'a1_271': 192, 'a1_272': 193, 'a1_273': 194, 'a1_274': 195, 'a1_275': 196, 'a1_276': 197, 'a1_277': 198, 'a1_278': 199, 'a1_279': 200, 'a1_28': 201, 'a1_280': 202, 'a1_281': 203, 'a1_282': 204, 'a1_283': 205, 'a1_284': 206, 'a1_285': 207, 'a1_286': 208, 'a1_287': 209, 'a1_288': 210, 'a1_289': 211, 'a1_29': 212, 'a1_290': 213, 'a1_291': 214, 'a1_292': 215, 'a1_293': 216, 'a1_294': 217, 'a1_295': 218, 'a1_296': 219, 'a1_297': 220, 'a1_298': 221, 'a1_299': 222, 'a1_3': 223, 'a1_30': 224, 'a1_300': 225, 'a1_301': 226, 'a1_302': 227, 'a1_303': 228, 'a1_304': 229, 'a1_305': 230, 'a1_306': 231, 'a1_307': 232, 'a1_308': 233, 'a1_309': 234, 'a1_31': 235, 'a1_310': 236, 'a1_311': 237, 'a1_312': 238, 'a1_313': 239, 'a1_314': 240, 'a1_315': 241, 'a1_316': 242, 'a1_317': 243, 'a1_318': 244, 'a1_319': 245, 'a1_32': 246, 'a1_320': 247, 'a1_321': 248, 'a1_322': 249, 'a1_323': 250, 'a1_324': 251, 'a1_325': 252, 'a1_326': 253, 'a1_327': 254, 'a1_328': 255, 'a1_329': 256, 'a1_33': 257, 'a1_330': 258, 'a1_331': 259, 'a1_332': 260, 'a1_333': 261, 'a1_334': 262, 'a1_335': 263, 'a1_336': 264, 'a1_337': 265, 'a1_338': 266, 'a1_339': 267, 'a1_34': 268, 'a1_340': 269, 'a1_341': 270, 'a1_342': 271, 'a1_343': 272, 'a1_344': 273, 'a1_345': 274, 'a1_346': 275, 'a1_347': 276, 'a1_348': 277, 'a1_349': 278, 'a1_35': 279, 'a1_350': 280, 'a1_351': 281, 'a1_352': 282, 'a1_353': 283, 'a1_354': 284, 'a1_355': 285, 'a1_356': 286, 'a1_357': 287, 'a1_358': 288, 'a1_359': 289, 'a1_36': 290, 'a1_360': 291, 'a1_361': 292, 'a1_362': 293, 'a1_363': 294, 'a1_364': 295, 'a1_365': 296, 'a1_366': 297, 'a1_367': 298, 'a1_368': 299, 'a1_369': 300, 'a1_37': 301, 'a1_370': 302, 'a1_371': 303, 'a1_372': 304, 'a1_373': 305, 'a1_374': 306, 'a1_375': 307, 'a1_376': 308, 'a1_377': 309, 'a1_378': 310, 'a1_379': 311, 'a1_38': 312, 'a1_380': 313, 'a1_381': 314, 'a1_382': 315, 'a1_383': 316, 'a1_384': 317, 'a1_385': 318, 'a1_386': 319, 'a1_387': 320, 'a1_388': 321, 'a1_389': 322, 'a1_39': 323, 'a1_390': 324, 'a1_391': 325, 'a1_392': 326, 'a1_393': 327, 'a1_394': 328, 'a1_395': 329, 'a1_396': 330, 'a1_397': 331, 'a1_398': 332, 'a1_399': 333, 'a1_4': 334, 'a1_40': 335, 'a1_400': 336, 'a1_401': 337, 'a1_402': 338, 'a1_403': 339, 'a1_404': 340, 'a1_405': 341, 'a1_406': 342, 'a1_407': 343, 'a1_408': 344, 'a1_409': 345, 'a1_41': 346, 'a1_410': 347, 'a1_411': 348, 'a1_412': 349, 'a1_413': 350, 'a1_414': 351, 'a1_415': 352, 'a1_416': 353, 'a1_417': 354, 'a1_418': 355, 'a1_419': 356, 'a1_42': 357, 'a1_420': 358, 'a1_421': 359, 'a1_422': 360, 'a1_423': 361, 'a1_424': 362, 'a1_425': 363, 'a1_426': 364, 'a1_427': 365, 'a1_428': 366, 'a1_429': 367, 'a1_43': 368, 'a1_430': 369, 'a1_431': 370, 'a1_432': 371, 'a1_433': 372, 'a1_434': 373, 'a1_435': 374, 'a1_436': 375, 'a1_437': 376, 'a1_438': 377, 'a1_439': 378, 'a1_44': 379, 'a1_440': 380, 'a1_441': 381, 'a1_442': 382, 'a1_443': 383, 'a1_444': 384, 'a1_445': 385, 'a1_446': 386, 'a1_447': 387, 'a1_448': 388, 'a1_449': 389, 'a1_45': 390, 'a1_450': 391, 'a1_451': 392, 'a1_452': 393, 'a1_453': 394, 'a1_454': 395, 'a1_455': 396, 'a1_456': 397, 'a1_457': 398, 'a1_458': 399, 'a1_459': 400, 'a1_46': 401, 'a1_460': 402, 'a1_461': 403, 'a1_462': 404, 'a1_463': 405, 'a1_464': 406, 'a1_465': 407, 'a1_466': 408, 'a1_467': 409, 'a1_468': 410, 'a1_469': 411, 'a1_47': 412, 'a1_470': 413, 'a1_471': 414, 'a1_472': 415, 'a1_473': 416, 'a1_474': 417, 'a1_475': 418, 'a1_476': 419, 'a1_477': 420, 'a1_478': 421, 'a1_479': 422, 'a1_48': 423, 'a1_480': 424, 'a1_481': 425, 'a1_482': 426, 'a1_483': 427, 'a1_484': 428, 'a1_485': 429, 'a1_486': 430, 'a1_487': 431, 'a1_488': 432, 'a1_489': 433, 'a1_49': 434, 'a1_490': 435, 'a1_491': 436, 'a1_492': 437, 'a1_493': 438, 'a1_494': 439, 'a1_495': 440, 'a1_496': 441, 'a1_497': 442, 'a1_498': 443, 'a1_499': 444, 'a1_5': 445, 'a1_50': 446, 'a1_500': 447, 'a1_501': 448, 'a1_502': 449, 'a1_503': 450, 'a1_504': 451, 'a1_505': 452, 'a1_506': 453, 'a1_507': 454, 'a1_508': 455, 'a1_509': 456, 'a1_51': 457, 'a1_510': 458, 'a1_511': 459, 'a1_512': 460, 'a1_513': 461, 'a1_514': 462, 'a1_515': 463, 'a1_516': 464, 'a1_517': 465, 'a1_518': 466, 'a1_519': 467, 'a1_52': 468, 'a1_520': 469, 'a1_521': 470, 'a1_522': 471, 'a1_523': 472, 'a1_524': 473, 'a1_525': 474, 'a1_526': 475, 'a1_527': 476, 'a1_528': 477, 'a1_529': 478, 'a1_53': 479, 'a1_530': 480, 'a1_531': 481, 'a1_532': 482, 'a1_533': 483, 'a1_534': 484, 'a1_535': 485, 'a1_536': 486, 'a1_537': 487, 'a1_538': 488, 'a1_539': 489, 'a1_54': 490, 'a1_540': 491, 'a1_541': 492, 'a1_542': 493, 'a1_543': 494, 'a1_544': 495, 'a1_545': 496, 'a1_546': 497, 'a1_547': 498, 'a1_548': 499, 'a1_549': 500, 'a1_55': 501, 'a1_550': 502, 'a1_551': 503, 'a1_552': 504, 'a1_553': 505, 'a1_554': 506, 'a1_555': 507, 'a1_556': 508, 'a1_557': 509, 'a1_558': 510, 'a1_559': 511, 'a1_56': 512, 'a1_560': 513, 'a1_561': 514, 'a1_562': 515, 'a1_563': 516, 'a1_564': 517, 'a1_565': 518, 'a1_566': 519, 'a1_567': 520, 'a1_568': 521, 'a1_569': 522, 'a1_57': 523, 'a1_570': 524, 'a1_571': 525, 'a1_572': 526, 'a1_573': 527, 'a1_574': 528, 'a1_575': 529, 'a1_576': 530, 'a1_577': 531, 'a1_578': 532, 'a1_579': 533, 'a1_58': 534, 'a1_580': 535, 'a1_581': 536, 'a1_582': 537, 'a1_583': 538, 'a1_584': 539, 'a1_585': 540, 'a1_586': 541, 'a1_587': 542, 'a1_588': 543, 'a1_589': 544, 'a1_59': 545, 'a1_590': 546, 'a1_591': 547, 'a1_592': 548, 'a1_593': 549, 'a1_594': 550, 'a1_595': 551, 'a1_596': 552, 'a1_597': 553, 'a1_598': 554, 'a1_599': 555, 'a1_6': 556, 'a1_60': 557, 'a1_600': 558, 'a1_601': 559, 'a1_602': 560, 'a1_603': 561, 'a1_604': 562, 'a1_605': 563, 'a1_606': 564, 'a1_607': 565, 'a1_608': 566, 'a1_609': 567, 'a1_61': 568, 'a1_610': 569, 'a1_611': 570, 'a1_612': 571, 'a1_613': 572, 'a1_614': 573, 'a1_615': 574, 'a1_616': 575, 'a1_617': 576, 'a1_618': 577, 'a1_619': 578, 'a1_62': 579, 'a1_620': 580, 'a1_621': 581, 'a1_622': 582, 'a1_623': 583, 'a1_624': 584, 'a1_625': 585, 'a1_626': 586, 'a1_627': 587, 'a1_628': 588, 'a1_629': 589, 'a1_63': 590, 'a1_630': 591, 'a1_631': 592, 'a1_632': 593, 'a1_633': 594, 'a1_634': 595, 'a1_635': 596, 'a1_636': 597, 'a1_637': 598, 'a1_638': 599, 'a1_639': 600, 'a1_64': 601, 'a1_640': 602, 'a1_641': 603, 'a1_642': 604, 'a1_643': 605, 'a1_644': 606, 'a1_645': 607, 'a1_646': 608, 'a1_647': 609, 'a1_648': 610, 'a1_649': 611, 'a1_65': 612, 'a1_650': 613, 'a1_651': 614, 'a1_652': 615, 'a1_653': 616, 'a1_654': 617, 'a1_655': 618, 'a1_656': 619, 'a1_657': 620, 'a1_658': 621, 'a1_659': 622, 'a1_66': 623, 'a1_660': 624, 'a1_661': 625, 'a1_662': 626, 'a1_663': 627, 'a1_664': 628, 'a1_665': 629, 'a1_666': 630, 'a1_667': 631, 'a1_668': 632, 'a1_669': 633, 'a1_67': 634, 'a1_670': 635, 'a1_671': 636, 'a1_672': 637, 'a1_673': 638, 'a1_674': 639, 'a1_675': 640, 'a1_676': 641, 'a1_677': 642, 'a1_678': 643, 'a1_679': 644, 'a1_68': 645, 'a1_680': 646, 'a1_681': 647, 'a1_682': 648, 'a1_683': 649, 'a1_684': 650, 'a1_685': 651, 'a1_686': 652, 'a1_687': 653, 'a1_688': 654, 'a1_689': 655, 'a1_69': 656, 'a1_690': 657, 'a1_691': 658, 'a1_692': 659, 'a1_693': 660, 'a1_694': 661, 'a1_695': 662, 'a1_696': 663, 'a1_697': 664, 'a1_698': 665, 'a1_699': 666, 'a1_7': 667, 'a1_70': 668, 'a1_700': 669, 'a1_701': 670, 'a1_702': 671, 'a1_703': 672, 'a1_704': 673, 'a1_705': 674, 'a1_706': 675, 'a1_707': 676, 'a1_708': 677, 'a1_709': 678, 'a1_71': 679, 'a1_710': 680, 'a1_711': 681, 'a1_712': 682, 'a1_713': 683, 'a1_714': 684, 'a1_715': 685, 'a1_716': 686, 'a1_717': 687, 'a1_718': 688, 'a1_719': 689, 'a1_72': 690, 'a1_720': 691, 'a1_721': 692, 'a1_722': 693, 'a1_723': 694, 'a1_724': 695, 'a1_725': 696, 'a1_726': 697, 'a1_727': 698, 'a1_728': 699, 'a1_729': 700, 'a1_73': 701, 'a1_730': 702, 'a1_731': 703, 'a1_732': 704, 'a1_733': 705, 'a1_734': 706, 'a1_735': 707, 'a1_736': 708, 'a1_737': 709, 'a1_738': 710, 'a1_739': 711, 'a1_74': 712, 'a1_740': 713, 'a1_741': 714, 'a1_742': 715, 'a1_743': 716, 'a1_744': 717, 'a1_745': 718, 'a1_746': 719, 'a1_747': 720, 'a1_748': 721, 'a1_749': 722, 'a1_75': 723, 'a1_750': 724, 'a1_751': 725, 'a1_752': 726, 'a1_753': 727, 'a1_754': 728, 'a1_755': 729, 'a1_756': 730, 'a1_757': 731, 'a1_758': 732, 'a1_759': 733, 'a1_76': 734, 'a1_760': 735, 'a1_761': 736, 'a1_762': 737, 'a1_763': 738, 'a1_764': 739, 'a1_765': 740, 'a1_766': 741, 'a1_767': 742, 'a1_768': 743, 'a1_769': 744, 'a1_77': 745, 'a1_770': 746, 'a1_771': 747, 'a1_772': 748, 'a1_773': 749, 'a1_774': 750, 'a1_775': 751, 'a1_776': 752, 'a1_777': 753, 'a1_778': 754, 'a1_779': 755, 'a1_78': 756, 'a1_780': 757, 'a1_781': 758, 'a1_782': 759, 'a1_783': 760, 'a1_784': 761, 'a1_785': 762, 'a1_786': 763, 'a1_787': 764, 'a1_788': 765, 'a1_789': 766, 'a1_79': 767, 'a1_790': 768, 'a1_791': 769, 'a1_792': 770, 'a1_793': 771, 'a1_794': 772, 'a1_795': 773, 'a1_796': 774, 'a1_797': 775, 'a1_798': 776, 'a1_799': 777, 'a1_8': 778, 'a1_80': 779, 'a1_800': 780, 'a1_801': 781, 'a1_802': 782, 'a1_803': 783, 'a1_804': 784, 'a1_805': 785, 'a1_806': 786, 'a1_807': 787, 'a1_808': 788, 'a1_809': 789, 'a1_81': 790, 'a1_810': 791, 'a1_811': 792, 'a1_812': 793, 'a1_813': 794, 'a1_814': 795, 'a1_815': 796, 'a1_816': 797, 'a1_817': 798, 'a1_818': 799, 'a1_819': 800, 'a1_82': 801, 'a1_820': 802, 'a1_821': 803, 'a1_822': 804, 'a1_823': 805, 'a1_824': 806, 'a1_825': 807, 'a1_826': 808, 'a1_827': 809, 'a1_828': 810, 'a1_829': 811, 'a1_83': 812, 'a1_830': 813, 'a1_831': 814, 'a1_832': 815, 'a1_833': 816, 'a1_834': 817, 'a1_835': 818, 'a1_836': 819, 'a1_837': 820, 'a1_838': 821, 'a1_839': 822, 'a1_84': 823, 'a1_840': 824, 'a1_841': 825, 'a1_842': 826, 'a1_843': 827, 'a1_844': 828, 'a1_845': 829, 'a1_846': 830, 'a1_847': 831, 'a1_848': 832, 'a1_849': 833, 'a1_85': 834, 'a1_850': 835, 'a1_851': 836, 'a1_852': 837, 'a1_853': 838, 'a1_854': 839, 'a1_855': 840, 'a1_856': 841, 'a1_857': 842, 'a1_858': 843, 'a1_859': 844, 'a1_86': 845, 'a1_860': 846, 'a1_861': 847, 'a1_862': 848, 'a1_863': 849, 'a1_864': 850, 'a1_865': 851, 'a1_866': 852, 'a1_867': 853, 'a1_868': 854, 'a1_869': 855, 'a1_87': 856, 'a1_870': 857, 'a1_871': 858, 'a1_872': 859, 'a1_873': 860, 'a1_874': 861, 'a1_875': 862, 'a1_876': 863, 'a1_877': 864, 'a1_878': 865, 'a1_879': 866, 'a1_88': 867, 'a1_880': 868, 'a1_881': 869, 'a1_882': 870, 'a1_883': 871, 'a1_884': 872, 'a1_885': 873, 'a1_886': 874, 'a1_887': 875, 'a1_888': 876, 'a1_889': 877, 'a1_89': 878, 'a1_890': 879, 'a1_891': 880, 'a1_892': 881, 'a1_893': 882, 'a1_894': 883, 'a1_895': 884, 'a1_896': 885, 'a1_897': 886, 'a1_898': 887, 'a1_899': 888, 'a1_9': 889, 'a1_90': 890, 'a1_900': 891, 'a1_901': 892, 'a1_902': 893, 'a1_903': 894, 'a1_904': 895, 'a1_905': 896, 'a1_906': 897, 'a1_907': 898, 'a1_908': 899, 'a1_909': 900, 'a1_91': 901, 'a1_910': 902, 'a1_911': 903, 'a1_912': 904, 'a1_913': 905, 'a1_914': 906, 'a1_915': 907, 'a1_916': 908, 'a1_917': 909, 'a1_918': 910, 'a1_919': 911, 'a1_92': 912, 'a1_920': 913, 'a1_921': 914, 'a1_922': 915, 'a1_923': 916, 'a1_924': 917, 'a1_925': 918, 'a1_926': 919, 'a1_927': 920, 'a1_928': 921, 'a1_929': 922, 'a1_93': 923, 'a1_930': 924, 'a1_931': 925, 'a1_932': 926, 'a1_933': 927, 'a1_934': 928, 'a1_935': 929, 'a1_936': 930, 'a1_937': 931, 'a1_938': 932, 'a1_939': 933, 'a1_94': 934, 'a1_940': 935, 'a1_941': 936, 'a1_942': 937, 'a1_943': 938, 'a1_944': 939, 'a1_945': 940, 'a1_946': 941, 'a1_947': 942, 'a1_948': 943, 'a1_949': 944, 'a1_95': 945, 'a1_950': 946, 'a1_951': 947, 'a1_952': 948, 'a1_953': 949, 'a1_954': 950, 'a1_955': 951, 'a1_956': 952, 'a1_957': 953, 'a1_958': 954, 'a1_959': 955, 'a1_96': 956, 'a1_960': 957, 'a1_961': 958, 'a1_962': 959, 'a1_963': 960, 'a1_964': 961, 'a1_965': 962, 'a1_966': 963, 'a1_967': 964, 'a1_968': 965, 'a1_969': 966, 'a1_97': 967, 'a1_970': 968, 'a1_971': 969, 'a1_972': 970, 'a1_973': 971, 'a1_974': 972, 'a1_975': 973, 'a1_976': 974, 'a1_977': 975, 'a1_978': 976, 'a1_979': 977, 'a1_98': 978, 'a1_980': 979, 'a1_981': 980, 'a1_982': 981, 'a1_983': 982, 'a1_984': 983, 'a1_985': 984, 'a1_986': 985, 'a1_987': 986, 'a1_988': 987, 'a1_989': 988, 'a1_99': 989, 'a1_990': 990, 'a1_991': 991, 'a1_992': 992, 'a1_993': 993, 'a1_994': 994, 'a1_995': 995, 'a1_996': 996, 'a1_997': 997, 'a1_998': 998, 'a1_999': 999}
    inv_map = {v:k for k,v in map.items()}
    answerssss=[]
    for p in answer:
        answerssss.append(inv_map[p])
    df=pd.read_csv("sample_submission.csv")
    df['label']=answerssss
    df.to_csv("submission1.csv",index=False)



if __name__ == '__main__':

    parser = argparse.ArgumentParser()

    parser.add_argument('--data-dir', type=str, default=None, help='path of /dataset/')
    parser.add_argument('--num-epochs', type=int, default=40)
    parser.add_argument('--batch-size', type=int, default=1, help='total batch size for all GPUs')
    parser.add_argument('--img-size', type=int, default=[64, 64], help='img sizes')
    parser.add_argument('--class-num', type=int, default=1000, help='class num')

    parser.add_argument('--weights-loc', type=str, default= None, help='path of weights (if going to be loaded)')

    parser.add_argument("--lr", type=float, default= 0.01, help="learning rate")
    parser.add_argument("--net-name", type=str, default="efficientnet-b3", help="efficientnet type")

    parser.add_argument('--resume-epoch', type=int, default=0, help='what epoch to start from')

    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')



    opt = parser.parse_args()

    data_dir = opt.data_dir
    num_epochs = opt.num_epochs
    batch_size = opt.batch_size
    input_size = opt.img_size
    class_num = opt.class_num

    weights_loc = opt.weights_loc

    lr = opt.lr
    net_name = opt.net_name

    epoch_to_resume_from = opt.resume_epoch

    momentum = opt.momentum

    print("data dir: ", data_dir, ",  num epochs: ", num_epochs, ",  batch size: ",batch_size,
             ", img size: ", input_size, ", num of classes:", class_num, ".pth weights file location:", weights_loc,
             ", learning rate:", lr, ", net name:", net_name, "epoch to resume from: ", epoch_to_resume_from, "momen")

    run()
